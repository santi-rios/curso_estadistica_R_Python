---
title: "Introducci贸n a Correlaci贸n de Pearson y Spearman"
author:
  - name: "Mtro. Santiago R铆os"
    email: santiagoboo99@gmail.com
    affiliation: 
      - name: Cursos Orca
        city: CDMX
        url: orcaasesina.com
format: 
    live-html:
        highlightStyle: github
        highlightLines: true
        theme: superhero
toc: true
sidebar: false
webr:
    packages: 
        - datos
        - dplyr
        - tidyr
        - ggplot2
    render-df: gt-interactive
engine: knitr
---

{{< include ../_extensions/r-wasm/live/_knitr.qmd >}}


Antes de comenzar con el tema, hay que introducir dos conceptos que estaremos revisando: **Pruebas de Hip贸tesis** y las **Hip贸tesis Nula y Alternativa**. Aunque todav铆a no vemos formalmente el tema de pruebas de hip贸tesis, es importante tener una idea general de estos conceptos para entender mejor la correlaci贸n como un modelo lineal.

::: {.callout-important}
## Pruebas de hip贸tesis

Una prueba de hip贸tesis es un procedimiento estad铆stico que se utiliza para tomar decisiones sobre una poblaci贸n basada en una muestra de datos. Es una herramienta fundamental para determinar si un resultado observado puede atribuirse al azar o si es significativo y refleja una determinada caracter铆stica de la poblaci贸n de la que se extrajo la muestra.

**Componentes de una Prueba de Hip贸tesis**

- Hip贸tesis Nula $H_0$: Una afirmaci贸n inicial que se presume verdadera hasta que se demuestre lo contrario. Generalmente, esta hip贸tesis propone que no hay efecto o no hay diferencia. Si el valor p es mayor o igual al nivel de significaci贸n, no se rechaza la hip贸tesis nula.

- Hip贸tesis Alternativa $H_a$ o $H_1$: Una afirmaci贸n contraria a la hip贸tesis nula. Propone que hay un efecto o una diferencia. Es lo que el investigador probablemente espera demostrar o encontrar evidencia a favor. Si el valor p es menor que el nivel de significaci贸n (usualmente 0.05), se rechaza la hip贸tesis nula.

- Estad铆stico de Prueba: Un valor calculado a partir de los datos de muestra que se utiliza para decidir si rechazar la hip贸tesis nula.

- Regla de Decisi贸n: Basada en el valor del estad铆stico de prueba y el nivel de significaci贸n ($\alpha$, normalmente 0.05), determina si se rechaza o no la hip贸tesis nula. M谩s adelante veremos a detalle de d贸nde sale este valor.

- Valor P: La probabilidad de observar un resultado tan extremo como, o m谩s extremo que, el observado en la muestra, si la hip贸tesis nula es verdadera. **NOTA** que el valor p NO es la probabilidad de que la hip贸tesis nula sea verdadera o falsa. Por ahora puede parecer confuso, pero es importante que no pienses que este valor es la probabilidad de que la hip贸tesis nula sea cierta o falsa. Si el valor p es menor que el nivel de significaci贸n, se rechaza $H_0$. M谩s adelante veremos a detalle de d贸nde sale este valor.

**Ejemplos de Pruebas de Hip贸tesis**

- Prueba de correlaci贸n de Pearson:
        Eval煤a si existe una relaci贸n lineal entre dos variables continuas.
        Ejemplo: Analizas si hay una correlaci贸n entre el 铆ndice de masa corporal (IMC) y la presi贸n arterial sist贸lica. La hip贸tesis nula es que el coeficiente de correlaci贸n es igual a cero (no hay correlaci贸n).

- Prueba t de Student:
        Se utiliza para comparar las medias de dos grupos diferentes.
        Ejemplo: Supongamos que deseas comparar si la media de la presi贸n arterial sist贸lica es diferente entre hombres y mujeres. La hip贸tesis nula ser铆a que la media es la misma para ambos grupos.

- Prueba ANOVA (An谩lisis de Varianza):
        Se utiliza para comparar las medias de tres o m谩s grupos.
        Ejemplo: Si tienes tres dietas diferentes y deseas comprobar si hay diferencias en los niveles de colesterol promedio de las personas que siguen cada dieta. La hip贸tesis nula es que todas las medias poblacionales son iguales.

## Hip贸tesis nula y alternativa
- En estad铆stica, hablamos mucho de "hip贸tesis". Esta es una afirmaci贸n que se hace sobre un par谩metro de una poblaci贸n.
- Por ejemplo, podemos tener la hip贸tesis que la correlaci贸n entre dos variables es cero (no hay relaci贸n entre ellas). 
- Tambi茅n podemos tener la hip贸tesis de que la media de una poblaci贸n es igual a un valor espec铆fico.
- Las hip贸tesis se prueban con pruebas estad铆sticas, y se pueden aceptar o rechazar en funci贸n de la evidencia.
- En las pruebas estad铆sticas, siempre se plantea una hip贸tesis nula ($H_0$) y una hip贸tesis alternativa ($H_1$).
- La hip贸tesis nula ($H_0$) es la afirmaci贸n que se somete a prueba, mientras que la hip贸tesis alternativa ($H_1$) es la afirmaci贸n que se acepta si se rechaza la hip贸tesis nula. 驴uhhh ? 
  - La hip贸tesis nula (denotada como $H_0$) es una afirmaci贸n general o un supuesto inicial que se considera verdadero hasta que se presente suficiente evidencia estad铆stica en su contra. En el contexto de la correlaci贸n entre dos variables, la hip贸tesis nula usualmente establece que **no hay efecto o relaci贸n entre las variables**. Por ejemplo, si est谩s analizando la correlaci贸n entre el 铆ndice de masa corporal (IMC) y la presi贸n arterial sist贸lica (PAS), la hip贸tesis nula podr铆a ser:
    - $H_0$: No hay correlaci贸n entre IMC y PAS
  - La hip贸tesis alternativa ($H_a$ o $H_1$) es la afirmaci贸n que se acepta si los datos proporcionan evidencia suficiente para rechazar la hip贸tesis nula. En el mismo ejemplo del IMC y la PAS, la hip贸tesis alternativa podr铆a ser:
    - $H_a$: Existe una correlaci贸n entre IMC y PAS. Esto significa que creemos que hay alguna relaci贸n entre las dos variables.
:::

---

## **Introducci贸n a la Correlaci贸n como un Modelo Lineal**

- La correlaci贸n de Pearson y Spearman son herramientas estad铆sticas fundamentales en el an谩lisis de datos. 
- Ambas correlaciones pueden interpretarse como **casos especiales de modelos lineales** simple, donde el objetivo es analizar la relaci贸n entre dos variables (como lo hicimos en el ejercicio pasado). 
- El concepto central de estas correlaciones se basa en un modelo de regresi贸n lineal simple:

$$
y = \beta_0 + \beta_1 \cdot x
$$

En este modelo, $y$ es la variable dependiente, $x$ es la variable independiente, $\beta_0$ es el intercepto (valor de $y$ cuando $x = 0$) y $\beta_1$ es la pendiente de la relaci贸n entre $x$ e $y$.


- **Correlaci贸n de Pearson**: Mide la relaci贸n lineal entre dos variables continuas.
  - $y = \beta_0 + \beta_1 \cdot x$
  - La correlaci贸n de Pearson eval煤a qu茅 tan fuerte es la relaci贸n lineal entre dos variables continuas. Es sensible a valores at铆picos y requiere que las variables sigan una distribuci贸n normal.
- **Correlaci贸n de Spearman**: Mide la relaci贸n entre los rangos de dos variables, lo que la hace adecuada para datos no param茅tricos o no lineales.
  - Eval煤a la **relaci贸n entre los rangos** de las dos variables, lo que la hace robusta frente a datos no normales y valores at铆picos. Se basa en la correlaci贸n de Pearson, pero aplicada a los rangos de las variables.
  - Tambi茅n es 煤til para datos ordinales: Cuando tratas con variables ordinales (datos que representan categor铆as con un orden natural), Spearman es m谩s adecuado que Pearson.

En ambas correlaciones, la hip贸tesis nula $H_0$ es que no hay relaci贸n entre las variables $\beta_1 = 0$.

---

::: {.callout-note}
## Rangos, pruebas no param茅tricas y la correlaci贸n de Spearman

La correlaci贸n de Spearman es una medida **no param茅trica** de la asociaci贸n entre dos variables. 

**Rango**

- En el contexto de la correlaci贸n de Spearman, el rango se refiere a la posici贸n de un dato en una secuencia ordenada. 
- Por ejemplo, si tienes un conjunto de datos, primero los ordenar铆as de menor a mayor y luego asignar铆as un n煤mero a su posici贸n en esta secuencia, es decir, su "rango". As铆, el dato m谩s peque帽o tendr铆a un rango de 1, el siguiente m谩s peque帽o un rango de 2, y as铆 sucesivamente.
- En la siguiente lecci贸n veremos m谩s sobre los rangos.

**Datos No Param茅tricos**
- Estos son datos que no necesariamente cumplen con los supuestos necesarios para m茅todos param茅tricos (como la distribuci贸n normal). 
- Las pruebas no param茅tricas no asumen que los datos sigan una distribuci贸n normal, lo cual es un requisito para muchas pruebas param茅tricas. 
- En lugar de basarse en los valores reales de los datos, las pruebas no param茅tricas se basan en los **rangos**, lo que les permite ser m谩s flexibles y aplicables a una variedad m谩s amplia de tipos de datos.

**Pruebas param茅tricas**

Se basan en asunciones espec铆ficas sobre la distribuci贸n de los datos de la poblaci贸n. Estas pruebas t铆picamente asumen que los datos son continuos, intervalos o m茅tricos y siguen una distribuci贸n normal (gaussiana). Ejemplos de pruebas param茅tricas incluyen:

** Resumen**

Mientras que las pruebas param茅tricas se sustentan en asunciones sobre la distribuci贸n de los datos y utilizan valores absolutos, las pruebas no param茅tricas son m谩s flexibles y suelen basarse en rangos para evaluar la evidencia en los datos sin asumir una distribuci贸n espec铆fica. Esto hace que las pruebas no param茅tricas sean m谩s adecuadas para datos ordinales, para datos con distribuciones no normales, o cuando los supuestos de las pruebas param茅tricas no se cumplen.

:::

---

## **Ejemplo en R: Correlaci贸n de Pearson y Spearman**

Utilizaremos un ejemplo pr谩ctico para ilustrar c贸mo calcular e interpretar las correlaciones de Pearson y Spearman utilizando **R** y paquetes del **tidyverse**.

### **Paso 1: Preparar el entorno y los datos**

Primero, cargamos las librer铆as necesarias y generamos datos simulados para dos variables correlacionadas.

```{webr}
#| edit: false
#| runbutton: false
#| warning: false

# Cargar librer铆as necesarias
library(tidyverse) 
library(MASS) # Para generar datos correlacionados
library(broom) # Para extraer coeficientes de modelos

# Fijar semilla para reproducibilidad
set.seed(40)

# Generar datos correlacionados
D_correlation <- MASS::mvrnorm(30, mu = c(0.9, 0.9), Sigma = matrix(c(1, 0.8, 0.8, 1), ncol = 2), empirical = TRUE) %>%
  as_tibble() %>%
  rename(X1 = V1, X2 = V2)

# Ver las primeras filas de los datos
head(D_correlation)
```

### **Paso 2: Visualizaci贸n de la Relaci贸n Lineal (Correlaci贸n de Pearson)**

Antes de calcular la correlaci贸n de Pearson, visualizamos la relaci贸n entre las dos variables con un gr谩fico de dispersi贸n y ajustamos una l铆nea de regresi贸n.

```{webr}
library(ggplot2)

# Ajustar modelo lineal para la correlaci贸n de Pearson
modelo_pearson <- lm(X2 ~ X1, data = D_correlation)

# Graficar la relaci贸n entre X1 y X2
P_pearson <- ggplot(D_correlation, aes(x = X1, y = X2)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue", lwd = 1) +
  labs(title = "Correlaci贸n de Pearson", x = "X1", y = "X2")

P_pearson
```

### **Paso 3: C谩lculo de la Correlaci贸n de Pearson**

Ahora, calculamos el coeficiente de correlaci贸n de Pearson y lo interpretamos.

- Para calcular la correlaci贸n de Pearson, usamos la funci贸n `cor()` con el argumento `method = "pearson"`.
  - Esta funci贸n toma los siguientes argumentos: `x` y `y`, que son las dos variables a comparar, y `method`, que especifica el tipo de correlaci贸n a calcular. Podemos usar `"pearson"` para la correlaci贸n de Pearson y `"spearman"` para la correlaci贸n de Spearman.
  - Nota que estamos usando una notaci贸n diferente para nombrar las variables x y y. En esta notaci贸n (que se utiliza en muchas funciones de R, aunque no en el tidyverse), se usa `nombre_del_data_frame$nombre_de_la_variable` para referirse a una variable en un data frame.
- El coeficiente de correlaci贸n de Pearson var铆a entre -1 y 1, donde 1 indica una correlaci贸n positiva perfecta, -1 una correlaci贸n negativa perfecta y 0 no hay correlaci贸n.

```{webr}
# Calcular la correlaci贸n de Pearson
cor_pearson <- cor(D_correlation$X1, D_correlation$X2, method = "pearson")

# Mostrar el valor de la correlaci贸n de Pearson
cor_pearson
```

### **Paso 4: C谩lculo de la Correlaci贸n de Spearman**

Para calcular la correlaci贸n de Spearman, simplemente usamos los rangos de los datos en lugar de los valores originales.

```{webr}
# Calcular la correlaci贸n de Spearman
cor_spearman <- cor(D_correlation$X1, D_correlation$X2, method = "spearman")

# Mostrar el valor de la correlaci贸n de Spearman
cor_spearman
```

### **Paso 5: Interpretaci贸n de los Resultados**

- **Correlaci贸n de Pearson**: El coeficiente nos dice qu茅 tan fuerte es la relaci贸n lineal entre dos variables. Un valor cercano a 1 indica una fuerte relaci贸n positiva.
- **Correlaci贸n de Spearman**: Nos indica qu茅 tan fuerte es la relaci贸n entre los **rangos** de las dos variables. Es 煤til cuando los datos no siguen una distribuci贸n normal o tienen relaciones no lineales.

---

### **Ejercicio Pr谩ctico**

1. **Ejercicio 1: Correlaci贸n entre Peso y Altura**

Usa el conjunto de datos `mtcars` y calcula la correlaci贸n de Pearson entre el peso del veh铆culo (`wt`) y el consumo de combustible (`mpg`).

```{webr}
#| exercise: 05_cor_ej1_1

# Cargar los datos mtcars
data(mtcars)


```

::: {.solution exercise="05_cor_ej1_1"}
::: {.callout-tip collapse="false"}

## Soluci贸n
```r
# Cargar los datos mtcars
data(mtcars)

# Calcular la correlaci贸n de Pearson entre peso (wt) y mpg
cor_mpg_wt_pearson <- cor(mtcars$wt, mtcars$mpg, method = "pearson")

# Calcular la correlaci贸n de Spearman entre peso (wt) y mpg
cor_mpg_wt_spearman <- cor(mtcars$wt, mtcars$mpg, method = "spearman")

# Mostrar los resultados de las correlaciones
cor_mpg_wt_pearson
cor_mpg_wt_spearman
```
:::
:::

2. **Ejercicio 2: Visualizaci贸n de la Relaci贸n entre Peso y Consumo de Combustible**

Crea un gr谩fico de dispersi贸n para visualizar la relaci贸n entre el peso y el consumo de combustible, e incluye una l铆nea de regresi贸n.

```{webr}
#| exercise: 05_cor_ej1_2


```

::: {.solution exercise="05_cor_ej1_2"}
::: {.callout-tip collapse="false"}

## Soluci贸n
```r
# Graficar la relaci贸n entre peso y mpg
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Relaci贸n entre Peso y Consumo de Combustible", x = "Peso (wt)", y = "Consumo de Combustible (mpg)")
```

---

:::
:::


## **Conclusi贸n**

La correlaci贸n de Pearson y Spearman son m茅todos poderosos para evaluar relaciones entre variables. La **correlaci贸n de Pearson** es ideal para relaciones lineales en datos continuos y normalmente distribuidos, mientras que la **correlaci贸n de Spearman** es m谩s robusta frente a datos no normales o relaciones no lineales. Ambos m茅todos pueden interpretarse como casos especiales de un **modelo lineal** simple, lo que unifica el concepto de correlaci贸n dentro del contexto m谩s amplio de la regresi贸n lineal.

