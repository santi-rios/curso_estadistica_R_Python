---
title: "Introducción a Correlación de Pearson y Spearman"
author:
  - name: "Mtro. Santiago Ríos"
    email: santiagoboo99@gmail.com
    affiliation: 
      - name: Cursos Orca
        city: CDMX
        url: orcaasesina.com
format: 
    live-html:
        highlightStyle: github
        highlightLines: true
        theme: superhero
toc: true
sidebar: false
webr:
    packages: 
        - datos
        - dplyr
        - tidyr
        - ggplot2
    render-df: gt-interactive
engine: knitr
---

{{< include ../_extensions/r-wasm/live/_knitr.qmd >}}


Antes de comenzar con el tema, hay que introducir dos conceptos que estaremos revisando: **Pruebas de Hipótesis** y las **Hipótesis Nula y Alternativa**. Aunque todavía no vemos formalmente el tema de pruebas de hipótesis, es importante tener una idea general de estos conceptos para entender mejor la correlación como un modelo lineal.

::: {.callout-important}
## Pruebas de hipótesis

Una prueba de hipótesis es un procedimiento estadístico que se utiliza para tomar decisiones sobre una población basada en una muestra de datos. Es una herramienta fundamental para determinar si un resultado observado puede atribuirse al azar o si es significativo y refleja una determinada característica de la población de la que se extrajo la muestra.

**Componentes de una Prueba de Hipótesis**

- Hipótesis Nula $H_0$: Una afirmación inicial que se presume verdadera hasta que se demuestre lo contrario. Generalmente, esta hipótesis propone que no hay efecto o no hay diferencia. Si el valor p es mayor o igual al nivel de significación, no se rechaza la hipótesis nula.

- Hipótesis Alternativa $H_a$ o $H_1$: Una afirmación contraria a la hipótesis nula. Propone que hay un efecto o una diferencia. Es lo que el investigador probablemente espera demostrar o encontrar evidencia a favor. Si el valor p es menor que el nivel de significación (usualmente 0.05), se rechaza la hipótesis nula.

- Estadístico de Prueba: Un valor calculado a partir de los datos de muestra que se utiliza para decidir si rechazar la hipótesis nula.

- Regla de Decisión: Basada en el valor del estadístico de prueba y el nivel de significación ($\alpha$, normalmente 0.05), determina si se rechaza o no la hipótesis nula. Más adelante veremos a detalle de dónde sale este valor.

- Valor P: La probabilidad de observar un resultado tan extremo como, o más extremo que, el observado en la muestra, si la hipótesis nula es verdadera. **NOTA** que el valor p NO es la probabilidad de que la hipótesis nula sea verdadera o falsa. Por ahora puede parecer confuso, pero es importante que no pienses que este valor es la probabilidad de que la hipótesis nula sea cierta o falsa. Si el valor p es menor que el nivel de significación, se rechaza $H_0$. Más adelante veremos a detalle de dónde sale este valor.

**Ejemplos de Pruebas de Hipótesis**

- Prueba de correlación de Pearson:
        Evalúa si existe una relación lineal entre dos variables continuas.
        Ejemplo: Analizas si hay una correlación entre el índice de masa corporal (IMC) y la presión arterial sistólica. La hipótesis nula es que el coeficiente de correlación es igual a cero (no hay correlación).

- Prueba t de Student:
        Se utiliza para comparar las medias de dos grupos diferentes.
        Ejemplo: Supongamos que deseas comparar si la media de la presión arterial sistólica es diferente entre hombres y mujeres. La hipótesis nula sería que la media es la misma para ambos grupos.

- Prueba ANOVA (Análisis de Varianza):
        Se utiliza para comparar las medias de tres o más grupos.
        Ejemplo: Si tienes tres dietas diferentes y deseas comprobar si hay diferencias en los niveles de colesterol promedio de las personas que siguen cada dieta. La hipótesis nula es que todas las medias poblacionales son iguales.

## Hipótesis nula y alternativa
- En estadística, hablamos mucho de "hipótesis". Esta es una afirmación que se hace sobre un parámetro de una población.
- Por ejemplo, podemos tener la hipótesis que la correlación entre dos variables es cero (no hay relación entre ellas). 
- También podemos tener la hipótesis de que la media de una población es igual a un valor específico.
- Las hipótesis se prueban con pruebas estadísticas, y se pueden aceptar o rechazar en función de la evidencia.
- En las pruebas estadísticas, siempre se plantea una hipótesis nula ($H_0$) y una hipótesis alternativa ($H_1$).
- La hipótesis nula ($H_0$) es la afirmación que se somete a prueba, mientras que la hipótesis alternativa ($H_1$) es la afirmación que se acepta si se rechaza la hipótesis nula. ¿uhhh 😢? 
  - La hipótesis nula (denotada como $H_0$) es una afirmación general o un supuesto inicial que se considera verdadero hasta que se presente suficiente evidencia estadística en su contra. En el contexto de la correlación entre dos variables, la hipótesis nula usualmente establece que **no hay efecto o relación entre las variables**. Por ejemplo, si estás analizando la correlación entre el índice de masa corporal (IMC) y la presión arterial sistólica (PAS), la hipótesis nula podría ser:
    - $H_0$: No hay correlación entre IMC y PAS
  - La hipótesis alternativa ($H_a$ o $H_1$) es la afirmación que se acepta si los datos proporcionan evidencia suficiente para rechazar la hipótesis nula. En el mismo ejemplo del IMC y la PAS, la hipótesis alternativa podría ser:
    - $H_a$: Existe una correlación entre IMC y PAS. Esto significa que creemos que hay alguna relación entre las dos variables.
:::

---

## **Introducción a la Correlación como un Modelo Lineal**

- La correlación de Pearson y Spearman son herramientas estadísticas fundamentales en el análisis de datos. 
- Ambas correlaciones pueden interpretarse como **casos especiales de modelos lineales** simple, donde el objetivo es analizar la relación entre dos variables (como lo hicimos en el ejercicio pasado). 
- El concepto central de estas correlaciones se basa en un modelo de regresión lineal simple:

$$
y = \beta_0 + \beta_1 \cdot x
$$

En este modelo, $y$ es la variable dependiente, $x$ es la variable independiente, $\beta_0$ es el intercepto (valor de $y$ cuando $x = 0$) y $\beta_1$ es la pendiente de la relación entre $x$ e $y$.


- **Correlación de Pearson**: Mide la relación lineal entre dos variables continuas.
  - $y = \beta_0 + \beta_1 \cdot x$
  - La correlación de Pearson evalúa qué tan fuerte es la relación lineal entre dos variables continuas. Es sensible a valores atípicos y requiere que las variables sigan una distribución normal.
- **Correlación de Spearman**: Mide la relación entre los rangos de dos variables, lo que la hace adecuada para datos no paramétricos o no lineales.
  - Evalúa la **relación entre los rangos** de las dos variables, lo que la hace robusta frente a datos no normales y valores atípicos. Se basa en la correlación de Pearson, pero aplicada a los rangos de las variables.
  - También es útil para datos ordinales: Cuando tratas con variables ordinales (datos que representan categorías con un orden natural), Spearman es más adecuado que Pearson.

En ambas correlaciones, la hipótesis nula $H_0$ es que no hay relación entre las variables $\beta_1 = 0$.

---

::: {.callout-note}
## Rangos, pruebas no paramétricas y la correlación de Spearman

La correlación de Spearman es una medida **no paramétrica** de la asociación entre dos variables. 

**Rango**

- En el contexto de la correlación de Spearman, el rango se refiere a la posición de un dato en una secuencia ordenada. 
- Por ejemplo, si tienes un conjunto de datos, primero los ordenarías de menor a mayor y luego asignarías un número a su posición en esta secuencia, es decir, su "rango". Así, el dato más pequeño tendría un rango de 1, el siguiente más pequeño un rango de 2, y así sucesivamente.
- En la siguiente lección veremos más sobre los rangos.

**Datos No Paramétricos**
- Estos son datos que no necesariamente cumplen con los supuestos necesarios para métodos paramétricos (como la distribución normal). 
- Las pruebas no paramétricas no asumen que los datos sigan una distribución normal, lo cual es un requisito para muchas pruebas paramétricas. 
- En lugar de basarse en los valores reales de los datos, las pruebas no paramétricas se basan en los **rangos**, lo que les permite ser más flexibles y aplicables a una variedad más amplia de tipos de datos.

**Pruebas paramétricas**

Se basan en asunciones específicas sobre la distribución de los datos de la población. Estas pruebas típicamente asumen que los datos son continuos, intervalos o métricos y siguen una distribución normal (gaussiana). Ejemplos de pruebas paramétricas incluyen:

** Resumen**

Mientras que las pruebas paramétricas se sustentan en asunciones sobre la distribución de los datos y utilizan valores absolutos, las pruebas no paramétricas son más flexibles y suelen basarse en rangos para evaluar la evidencia en los datos sin asumir una distribución específica. Esto hace que las pruebas no paramétricas sean más adecuadas para datos ordinales, para datos con distribuciones no normales, o cuando los supuestos de las pruebas paramétricas no se cumplen.

:::

---

## **Ejemplo en R: Correlación de Pearson y Spearman**

Utilizaremos un ejemplo práctico para ilustrar cómo calcular e interpretar las correlaciones de Pearson y Spearman utilizando **R** y paquetes del **tidyverse**.

### **Paso 1: Preparar el entorno y los datos**

Primero, cargamos las librerías necesarias y generamos datos simulados para dos variables correlacionadas.

```{webr}
#| edit: false
#| runbutton: false
#| warning: false

# Cargar librerías necesarias
library(tidyverse) 
library(MASS) # Para generar datos correlacionados
library(broom) # Para extraer coeficientes de modelos

# Fijar semilla para reproducibilidad
set.seed(40)

# Generar datos correlacionados
D_correlation <- MASS::mvrnorm(30, mu = c(0.9, 0.9), Sigma = matrix(c(1, 0.8, 0.8, 1), ncol = 2), empirical = TRUE) %>%
  as_tibble() %>%
  rename(X1 = V1, X2 = V2)

# Ver las primeras filas de los datos
head(D_correlation)
```

### **Paso 2: Visualización de la Relación Lineal (Correlación de Pearson)**

Antes de calcular la correlación de Pearson, visualizamos la relación entre las dos variables con un gráfico de dispersión y ajustamos una línea de regresión.

```{webr}
library(ggplot2)

# Ajustar modelo lineal para la correlación de Pearson
modelo_pearson <- lm(X2 ~ X1, data = D_correlation)

# Graficar la relación entre X1 y X2
P_pearson <- ggplot(D_correlation, aes(x = X1, y = X2)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue", lwd = 1) +
  labs(title = "Correlación de Pearson", x = "X1", y = "X2")

P_pearson
```

### **Paso 3: Cálculo de la Correlación de Pearson**

Ahora, calculamos el coeficiente de correlación de Pearson y lo interpretamos.

- Para calcular la correlación de Pearson, usamos la función `cor()` con el argumento `method = "pearson"`.
  - Esta función toma los siguientes argumentos: `x` y `y`, que son las dos variables a comparar, y `method`, que especifica el tipo de correlación a calcular. Podemos usar `"pearson"` para la correlación de Pearson y `"spearman"` para la correlación de Spearman.
  - Nota que estamos usando una notación diferente para nombrar las variables x y y. En esta notación (que se utiliza en muchas funciones de R, aunque no en el tidyverse), se usa `nombre_del_data_frame$nombre_de_la_variable` para referirse a una variable en un data frame.
- El coeficiente de correlación de Pearson varía entre -1 y 1, donde 1 indica una correlación positiva perfecta, -1 una correlación negativa perfecta y 0 no hay correlación.

```{webr}
# Calcular la correlación de Pearson
cor_pearson <- cor(D_correlation$X1, D_correlation$X2, method = "pearson")

# Mostrar el valor de la correlación de Pearson
cor_pearson
```

### **Paso 4: Cálculo de la Correlación de Spearman**

Para calcular la correlación de Spearman, simplemente usamos los rangos de los datos en lugar de los valores originales.

```{webr}
# Calcular la correlación de Spearman
cor_spearman <- cor(D_correlation$X1, D_correlation$X2, method = "spearman")

# Mostrar el valor de la correlación de Spearman
cor_spearman
```

### **Paso 5: Interpretación de los Resultados**

- **Correlación de Pearson**: El coeficiente nos dice qué tan fuerte es la relación lineal entre dos variables. Un valor cercano a 1 indica una fuerte relación positiva.
- **Correlación de Spearman**: Nos indica qué tan fuerte es la relación entre los **rangos** de las dos variables. Es útil cuando los datos no siguen una distribución normal o tienen relaciones no lineales.

---

### **Ejercicio Práctico**

1. **Ejercicio 1: Correlación entre Peso y Altura**

Usa el conjunto de datos `mtcars` y calcula la correlación de Pearson entre el peso del vehículo (`wt`) y el consumo de combustible (`mpg`).

```{webr}
#| exercise: 05_cor_ej1_1

# Cargar los datos mtcars
data(mtcars)


```

::: {.solution exercise="05_cor_ej1_1"}
::: {.callout-tip collapse="false"}

## Solución
```r
# Cargar los datos mtcars
data(mtcars)

# Calcular la correlación de Pearson entre peso (wt) y mpg
cor_mpg_wt_pearson <- cor(mtcars$wt, mtcars$mpg, method = "pearson")

# Calcular la correlación de Spearman entre peso (wt) y mpg
cor_mpg_wt_spearman <- cor(mtcars$wt, mtcars$mpg, method = "spearman")

# Mostrar los resultados de las correlaciones
cor_mpg_wt_pearson
cor_mpg_wt_spearman
```
:::
:::

2. **Ejercicio 2: Visualización de la Relación entre Peso y Consumo de Combustible**

Crea un gráfico de dispersión para visualizar la relación entre el peso y el consumo de combustible, e incluye una línea de regresión.

```{webr}
#| exercise: 05_cor_ej1_2


```

::: {.solution exercise="05_cor_ej1_2"}
::: {.callout-tip collapse="false"}

## Solución
```r
# Graficar la relación entre peso y mpg
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Relación entre Peso y Consumo de Combustible", x = "Peso (wt)", y = "Consumo de Combustible (mpg)")
```

---

:::
:::


## **Conclusión**

La correlación de Pearson y Spearman son métodos poderosos para evaluar relaciones entre variables. La **correlación de Pearson** es ideal para relaciones lineales en datos continuos y normalmente distribuidos, mientras que la **correlación de Spearman** es más robusta frente a datos no normales o relaciones no lineales. Ambos métodos pueden interpretarse como casos especiales de un **modelo lineal** simple, lo que unifica el concepto de correlación dentro del contexto más amplio de la regresión lineal.

