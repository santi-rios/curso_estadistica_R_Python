---
title: "Supuestos de los Modelos Lineales"
author:
  - name: "Mtro. Santiago R칤os"
    email: santiagoboo99@gmail.com
    affiliation: 
      - name: Cursos Orca
        city: CDMX
        url: orcaasesina.com
format: 
    live-html:
        # highlightStyle: github
        # highlightLines: true
        theme: lux
toc: true
sidebar: false
webr:
    packages: 
        - datos
        - dplyr
        - tidyr
        - ggplot2
    render-df: gt-interactive
engine: knitr
---

{{< include ../_extensions/r-wasm/live/_knitr.qmd >}}



Un **supuesto** en estad칤stica es una condici칩n que debe cumplirse para que un modelo o m칠todo sea v치lido. Para que un modelo lineal funcione correctamente se deben cumplir estos **supuestos**:

## Linealidad

La relaci칩n entre el predictor $x$ y la variable respuesta $y$ es lineal. Es decir, el cambio en $y$ debido a un cambio en $x$ es constante.

::: {.callout-important}
## Linealidad NO es lo mismo que recta

Linealidad en el contexto de los modelos lineales no significa necesariamente que la relaci칩n entre las variables deba formar una l칤nea recta en el gr치fico. En lugar de referirse a la forma de la gr치fica, la linealidad en los modelos lineales se refiere a c칩mo los par치metros (o coeficientes) del modelo se combinan con las variables predictoras. Por ejemplo, consider la siguiente ecuaci칩n de un modelo lineal:

$$
y = \beta_0 + \beta_1 \cdot x^4
$$

- Aunque la relaci칩n entre $x$ y $y$ no es una l칤nea recta, el modelo sigue siendo lineal porque los coeficientes $\beta_0$ y $\beta_1$ se combinan de manera lineal con $x$.

```{webr}
#| edit: false
#| runbutton: false

# definir los componentes del modelo lineal
beta_0 <- 5 # intercepto, cuando x = 0
beta_1 <- 2 # pendiente, cambio en y por cada unidad de cambio en x

# generar datos para x
x <- seq(0, 10, by = 0.5) # valores de x de 0 a 10

# calcular y a partir del modelo lineal
y <- beta_0 + beta_1 * x^4   # valores de y seg칰n el modelo lineal

# visualizar los datos y el modelo lineal
plot(x, y, type = "l", col = "blue", lwd = 2, xlab = "x", ylab = "y", main = "Modelo Lineal con x^4")


```

- 쮺칩mo se ver칤a una relaci칩n NO lineal? Si los coeficientes est치n involucrados en operaciones no lineales (exponenciales o elevados al cuadrado), perder칤amos la linealidad del modelo. Por ejemplo:

$$
y = e^{\beta_0 + \beta_1 \cdot x}
$$
:::

## Residuos

Los residuos de un modelo lineal deben cumplir ciertos supuestos. Antes de verlos, vamos a definir qu칠 son los residuos en un modelo lineal.



- Los **residuos** en un modelo lineal son las diferencias entre los valores observados (p. ej. los que obtenemos en nuestro experimento) y los valores predichos por el modelo. 
- Estos residuos son una medida de cu치nto se desv칤an los datos reales de la l칤nea de regresi칩n ajustada.
- Los residuos son una parte clave del modelo, ya que representan el error o la incertidumbre que no puede ser explicada por las variables predictoras. Un buen modelo de regresi칩n tendr치 residuos peque침os y distribuidos de manera aleatoria alrededor de la l칤nea de regresi칩n. 
- Vamos a ver esto con ejemplos para que quede m치s claro.


### Ejemplo

- Vamos a ver c칩mo se ven los residuos en un modelo lineal.
- Primero, vamos a ajustar un modelo de regresi칩n lineal simple con datos simulados.
- En el siguiente gr치fico, la l칤nea roja representa la regresi칩n lineal, los puntos naranjas son los datos observados (p.ej. los datos que obtuvimos en un experimento). 
- Las l칤neas negras representan los residuos. Observa que los residuos son las distancias verticales entre los puntos observados y la l칤nea de regresi칩n. 


::: {.callout collapse="true"}
## Click para ver el c칩digo usado en el gr치fico


```{webr}
#| edit: false
#| runbutton: false
#| warning: false

# Instalar y cargar ggplot2 si no est치 instalado
# install.packages("ggplot2")
library(ggplot2)

# Generar un conjunto de datos de ejemplo
set.seed(123)  # Para reproducibilidad
n <- 10  # N칰mero de puntos de datos
x <- seq(1, 10, length.out = n)
y <- 2 + 3 * x + rnorm(n, mean = 0, sd = 2)  # y = 2 + 3x + error aleatorio

# Ajustar un modelo de regresi칩n lineal
model <- lm(y ~ x)

# Crear un dataframe con los valores predichos y residuos
data <- data.frame(
  x = x,
  y = y,
  y_pred = predict(model),  # Valores predichos por el modelo
  resid = resid(model)  # Residuos (y_observado - y_predicho)
)

# Crear el gr치fico
residuos_simulados <- ggplot(data, aes(x = x, y = y)) +
  # L칤nea de regresi칩n
  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], 
              color = "red", linetype = "dashed", size = 1) +
  
  # Puntos observados
  geom_point(color = "orange", size = 3) +
  
  # L칤neas de los residuos
  geom_segment(aes(xend = x, yend = y_pred), linetype = "dashed", color = "black") +
  
  # Etiquetas de los residuos en el eje y
  # geom_text(aes(label = round(resid, 2), y = (y + y_pred) / 2), 
   #         color = "black", vjust = -0.5, hjust = -0.5) +
  
  # Etiquetas de los puntos observados
  # geom_text(aes(label = round(y, 2)), vjust = -1.5, color = "orange") +
  
  # Etiquetas de los puntos predichos
  # geom_text(aes(y = y_pred, label = round(y_pred, 2)), vjust = 1.5, color = "blue") +
  
  # T칤tulo y etiquetas de los ejes
  labs(title = "Modelo de Regresi칩n Lineal y Residuos",
       x = "Variable Independiente (X)",
       y = "Variable Dependiente (y)") +
  
  # Tema personalizado para mejorar la visualizaci칩n
  theme_classic()

```
:::

```{webr}
#| edit: false
#| runbutton: false
#| warning: false

print(residuos_simulados)
```


- Regresemos al ejemplo de la relaci칩n entre la masa corporal y el largo de la aleta de los ping칲inos. 
- Vamos a ajustar un modelo de regresi칩n lineal y visualizar los residuos de todos los datos.
- La l칤nea roja representa la regresi칩n lineal, los puntos naranjas son los datos observados y las l칤neas negras representan los residuos. 

::: {.callout collapse="true"}
## Click para ver el c칩digo usado en el gr치fico


```{webr}
#| edit: false
#| runbutton: false
#| warning: false

library(datos)
library(ggplot2)

# Eliminar filas con valores faltantes
pinguinos_clean <- na.omit(pinguinos[, c("masa_corporal_g", "largo_aleta_mm")])

# Ajustar el modelo de regresi칩n lineal
model <- lm(largo_aleta_mm ~ masa_corporal_g, data = pinguinos_clean)

# Crear un dataframe con los valores predichos y residuos
datos_valores <- data.frame(
  x = pinguinos_clean$masa_corporal_g,
  y = pinguinos_clean$largo_aleta_mm,
  y_pred = predict(model),  # Valores predichos por el modelo
  resid = resid(model)  # Residuos (y_observado - y_predicho)
)

# Crear el gr치fico

residuos_pinguinos <- ggplot(datos_valores, aes(x = x, y = y)) +
  # L칤nea de regresi칩n
  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], 
              color = "red", linetype = "dashed", size = 1) +
  
  # Puntos observados
  geom_point(color = "orange", size = 3) +
  
  # L칤neas de los residuos
  geom_segment(aes(xend = x, yend = y_pred), linetype = "dashed", color = "black") +
  
  # Etiquetas de los residuos en el eje y
  # geom_text(aes(label = round(resid, 2), y = (y + y_pred) / 2), 
    #        color = "black", vjust = -0.5, hjust = -0.5) +
  
  # Etiquetas de los puntos observados
  # geom_text(aes(label = round(y, 2)), vjust = -1.5, color = "orange") +
  
  # Etiquetas de los puntos predichos
  # geom_text(aes(y = y_pred, label = round(y_pred, 2)), vjust = 1.5, color = "blue") +
  
  # T칤tulo y etiquetas de los ejes
  labs(title = "Modelo de Regresi칩n Lineal y Residuos - Pinguinos 游냖",
       x = "Masa Corporal (g)",
       y = "Largo de la Aleta (mm)") +
  
  # Tema personalizado para mejorar la visualizaci칩n
  theme_classic()

```
:::

```{webr}
#| edit: false
#| runbutton: false
#| warning: false

print(residuos_pinguinos)
```







::: {.callout-important}
- 쮺칩mo se ajusta la recta de regresi칩n a los datos?
  - La recta de regresi칩n se ajusta minimizando la suma de los cuadrados de los residuos. Es decir, la recta se ajusta de tal manera que la suma de las distancias verticales entre los puntos observados y la recta es la menor posible. 
  - Esto se conoce como el **m칠todo de los m칤nimos cuadrados**.
  - Observa la siguiente figura para entender c칩mo se ajusta la recta de regresi칩n a los datos:

![La ssuma de cuadrados trata de minimizzar el 치rea total de los cuadrados grises que se observan en la derecha](https://kenndanielso.github.io/mlrefined/mlrefined_images/superlearn_images/Least_Squares.png)
:::


### Supuesto de residuos

***Normalidad***

Los **residuos** (diferencias entre los valores observados y los predichos por el modelo) deben seguir una **distribuci칩n normal**. Esto significa que los residuos deben estar distribuidos sim칠tricamente alrededor de cero y seguir una forma de campana similar a la distribuci칩n normal, como en el siguiente gr치fico:

![](https://qph.cf2.quoracdn.net/main-qimg-426f562fb3bdf0e65f22844b916ac29a.webp)


***Homocedasticidad***

La homocedadasticidad es la propiedad de los residuos de un modelo lineal de tener una **varianza constante** a lo largo de todos los niveles de los predictores. En otras palabras, la **dispersi칩n** de los residuos debe ser constante en todos los valores de las variables predictoras. Lo contrario es llamado **heterocedasticidad**.

> La **varianza de los residuos** es constante para todos los valores de las variables predictoras. En otras palabras, el **ruido** en las predicciones del modelo debe ser el mismo a lo largo de todos los niveles de los predictores.


::: { layout-ncol="2"}

![Homocedasticidad](https://www.lifeder.com/wp-content/uploads/2019/12/homocedasticidad-01.jpg)

![Heterocedasticidad. En valores bajos de X, hay poca varianza de los residuos, mientras que en valores altos, hay mucha m치s varianza. Esto se obsreva como un cono.](https://www.investopedia.com/thmb/mS-TpQisYn2qIDHExb3KbnceISw=/750x0/filters:no_upscale():max_bytes(150000):strip_icc()/Heteroskedasticity22-ce5acc2acef6494d91935588b0599579.png)
:::

### Verificaci칩n de los supuestos de los residuos

- Para verificar la normalidad de los residuos, se pueden utilizar pruebas estad칤sticas como la **prueba de Shapiro-Wilk** o gr치ficos como el **gr치fico Q-Q** o gr치ficos de **residuos**.
- Vamos a ver los gr치ficos primero.

> Primero, tenemos que ajustar el modelo lineal. Vamos a hacerlo con los datos de los ping칲inos. En la lecci칩n pasada vimos la funci칩n `lm()` para ajustar un modelo lineal. Vamos a hacerlo de nuevo.
> Esta funci칩n tomar치 como argumentos la f칩rmula del modelo y los datos donde se encuentran las variables de la siguiente manera: `lm(y ~ x, data = datos)`.
> En este caso, la variable respuesta es el largo de la aleta (`largo_aleta_mm`) y la variable predictora es la masa corporal (`masa_corporal_g`).
> Ajusta el modelo y guarda el resultado en una variable llamada `modelo_pinguinos`. Los datos `pinguinos_clean` ya est치n cargados y limpios (sin valores faltantes).


```{webr}
#| exercise: intro_lm_pinguino

# Ajustar el modelo de regresi칩n lineal
modelo_pinguinos <- lm(______ ~ ______, data = pinguinos_clean)
```

::: { .hint exercise="intro_lm_pinguino"}
::: { .callout-note collapse="false"}

## Hint 1

Recuerda que la variable predictora es aquella que queremos usar para predecir la variable respuesta. 

:::
:::

::: { .solution exercise="intro_lm_pinguino"}
::: { .callout-note collapse="false"}

## Soluci칩n

```r	
modelo_pinguinos <- lm(largo_aleta_mm ~ masa_corporal_g, data = pinguinos_clean)

```

:::
:::

#### gr치fico de residuos

- Ahora, para hacer el gr치fico de residuos con ggplot, vamos a hacer un gr치fico de dispersi칩n con los valores ajustados de la variable predictora en el eje x (los valores predichos por el modelo = `.fitted`) y los residuos en el eje y (las diferencias entre los valores observados = `.resid`). El dataframe es el modelo que ajustamos en la celda anterior.
- En el valor de cero en el eje x, los residuos deber칤an estar distribuidos aleatoriamente alrededor de la l칤nea horizontal en 0. Esto indicar칤a que los residuos siguen una distribuci칩n normal y que el supuesto de normalidad se cumple.


```{webr}
#| include: false
modelo_pinguinos <- lm(largo_aleta_mm ~ masa_corporal_g, data = pinguinos_clean)
```



```{webr}
#| warning: false
#| caption: El eje X representa los valores ajustados, es decir, los valores predichos por el modelo. El eje Y representa los residuos, es decir, las diferencias entre los valores observados y los predichos por el modelo.

library(ggplot2)

ggplot(modelo_pinguinos, aes(x = .fitted, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0) +
    labs(title = "Gr치fico de Residuos 游냖",
         x = "Valores Predichos",
         y = "Residuos") +
    theme_classic()

```


***C칩mo nos ayuda este gr치fico a verificar los supuestos de los residuos?***

- Si los residuos siguen una distribuci칩n normal, esperar칤amos que los puntos est칠n distribuidos aleatoriamente alred y alrededor de la l칤nea horizontal en 0.
- Si hay patrones en la distribuci칩n de los residuos (p.ej. forma de embudo, curvas, etc.), esto podr칤a indicar que los residuos no siguen una distribuci칩n normal y que el supuesto de normalidad no se cumple.
- Si la varianza de los residuos cambia a lo largo de los valores ajustados, esto podr칤a indicar que el supuesto de homocedasticidad no se cumple.
- En el gr치fico anterior, 쯤u칠 observas sobre la normalidad y homocedasticidad de los residuos?

Observa ejemplos de gr치ficos de residuos donde se cumplen y no se cumplen los supuestos de normalidad y homocedasticidad:

::: { layout-ncol="2"}

![Linearidad es v치lida porque la media de los residuos est치 cercana a 0 y la dispersi칩n de los residuos es similar](https://book.stat420.org/diagnostics_files/figure-html/unnamed-chunk-5-1.png)

![Linearidad es v치lida, pero la varianza no es constante: en valores altos de valores predichos, hay mucha m치s dispersi칩n. La figura de cono que observamos es ](https://book.stat420.org/diagnostics_files/figure-html/unnamed-chunk-7-1.png)

![El supuesto de linearidad no parece cumplirse porque la media de los residuos no est치 cercana a cero](https://book.stat420.org/diagnostics_files/figure-html/unnamed-chunk-9-1.png)

:::

#### gr치fico Q-Q

- Otra forma de verificar la normalidad de los residuos es mediante un gr치fico Q-Q (cuantil-cuantil). Este gr치fico compara los cuantiles de los residuos con los cuantiles de una distribuci칩n normal. Si los residuos siguen una distribuci칩n normal, los puntos en el gr치fico Q-Q deber칤an seguir una l칤nea diagonal.
- En las siguientes figuras, el gr치fico Q-Q de la izquierda muestra una distribuci칩n normal de los residuos, mientras que el de la derecha muestra una distribuci칩n no normal.


::: { layout-ncol="2"}

![](https://book.stat420.org/diagnostics_files/figure-html/unnamed-chunk-63-1.png)

![](https://book.stat420.org/diagnostics_files/figure-html/unnamed-chunk-65-1.png)

:::

Ahora, para hacer el gr치fico Q-Q de los residuos del modelo de los ping칲inos, vamos a usar la funci칩n `stat_qq()` y `stat_qq_line` de ggplot.

```{webr}

# Gr치fico Q-Q de los residuos
ggplot(modelo_pinguinos, aes(sample = .resid)) +
    stat_qq() +
    stat_qq_line() +
    labs(title = "Gr치fico Q-Q de los Residuos 游냖",
         x = "Cuantiles Te칩ricos",
         y = "Cuantiles de los Residuos") +
    theme_classic()

```


#### Reflexi칩n

- 쯉on o no son normales los residuos del modelo de regresi칩n lineal de los ping칲inos?
- Regresaremos a esta discusi칩n al final de este tema.
- Podemos contestar esta pregunta con una prueba estad칤stica, como la prueba de Shapiro-Wilk, que eval칰a si los residuos siguen una distribuci칩n normal, pero esto se ver치 despu칠s de que veamos qu칠 son las pruebas de hip칩tesis.
- Cuando los supuestos no se cumplen, el modelo puede proporcionar **predicciones sesgadas** o **intervalos de confianza incorrectos**, los cuales tambi칠n veremos m치s adelante.
- Sin embargo, existen t칠cnicas para manejar o ajustar estos problemas, como transformaciones de los datos o el uso de modelos m치s robustos.


#### Evaluaci칩n del Modelo

Veremos esto en detalle en futuras lecciones, pero es importante mencionar que una vez que hemos ajustado un modelo lineal, es necesario **evaluar su desempe침o**. Algunas de las m칠tricas m치s comunes incluyen:

- $R^2$ : Tambi칠n conocido como el **coeficiente de determinaci칩n**, mide la proporci칩n de la **variabilidad en $y$** que es explicada por el modelo. Un valor de $R^2$ cercano a 1 indica que el modelo explica bien los datos, mientras que un valor cercano a 0 sugiere que el modelo no captura mucha de la variabilidad en la variable respuesta.
- **Pruebas de hip칩tesis**: Se utilizan para evaluar la significancia de los coeficientes del modelo. Un **p-valor** bajo para un coeficiente $\beta_i$ sugiere que la variable $x$ tiene un impacto significativo sobre $y$.
- **An치lisis de residuos**: Verificar si los residuos siguen una distribuci칩n normal y si su varianza es constante puede ayudar a diagnosticar problemas con el ajuste del modelo.
